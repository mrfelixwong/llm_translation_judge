#!/usr/bin/env python3
"""
Main reliability study experiment.

This script runs the comprehensive study comparing all four judge techniques
across multiple language pairs and evaluation metrics.
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import json
import time
import argparse
from typing import Dict, List, Any
from datetime import datetime
import pandas as pd
from tqdm import tqdm

from src.judges import BasicJudge, FewShotJudge, MultiDimensionalJudge, MultiDimensionalJudgeOriginal, BackTranslationJudge
from src.data.test_sets import load_test_set, get_available_language_pairs
from src.evaluators.metrics import ReliabilityMetrics
from src.evaluators.benchmarks import GroundTruthEvaluator


class ReliabilityStudy:
    """
    Comprehensive reliability study for LLM judge techniques.
    
    Runs all four judge types on standardized test sets and compares:
    - Accuracy vs human expert judgments
    - Inter-judge consistency
    - Error detection rates
    - Cost-effectiveness
    """
    
    def __init__(self, output_dir: str = "results"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # Initialize judges
        self.judges = {
            "basic": BasicJudge(),
            "few_shot": FewShotJudge(),
            "multi_dimensional": MultiDimensionalJudge(),
            "multi_dimensional_original": MultiDimensionalJudgeOriginal(),
            "back_translation": BackTranslationJudge()
        }
        
        # Initialize evaluators
        self.reliability_metrics = ReliabilityMetrics()
        # Create a mock benchmark for ground truth evaluation
        from src.evaluators.benchmarks import TranslationBenchmark
        benchmark = TranslationBenchmark()
        from src.evaluators.benchmarks import GroundTruthEvaluator
        self.ground_truth_evaluator = GroundTruthEvaluator(benchmark)
        
        # Results storage
        self.results = {}
        self.metadata = {
            "start_time": None,
            "end_time": None,
            "total_evaluations": 0,
            "language_pairs": [],
            "test_cases": 0
        }
    
    def run_study(
        self,
        language_pairs: List[str] = None,
        test_size: int = 50,
        repetitions: int = 3,
        save_intermediate: bool = True
    ) -> Dict[str, Any]:
        """
        Run the complete reliability study.
        
        Args:
            language_pairs: List of language pairs to test (e.g., ['en-es', 'en-fr'])
            test_size: Number of test cases per language pair
            repetitions: Number of repetitions for consistency testing
            save_intermediate: Whether to save intermediate results
            
        Returns:
            Complete study results
        """
        
        print("üî¨ Starting LLM Judge Reliability Study")
        print("=" * 60)
        
        self.metadata["start_time"] = datetime.now().isoformat()
        
        if language_pairs is None:
            language_pairs = ["en-es", "en-fr", "en-ja"]  # Default test pairs
        
        self.metadata["language_pairs"] = language_pairs
        self.metadata["test_cases"] = test_size * len(language_pairs)
        
        total_evaluations = len(self.judges) * test_size * len(language_pairs) * repetitions
        self.metadata["total_evaluations"] = total_evaluations
        
        print(f"üìä Study Configuration:")
        print(f"   Language pairs: {language_pairs}")
        print(f"   Test cases per pair: {test_size}")
        print(f"   Repetitions: {repetitions}")
        print(f"   Total evaluations: {total_evaluations}")
        print()
        
        # Run evaluation for each language pair
        for lang_pair in language_pairs:
            print(f"üåç Testing language pair: {lang_pair}")
            self.results[lang_pair] = self._evaluate_language_pair(
                lang_pair, test_size, repetitions
            )
            
            if save_intermediate:
                self._save_intermediate_results(lang_pair)
        
        # Aggregate results
        aggregated_results = self._aggregate_results()
        
        # Calculate final metrics
        final_metrics = self._calculate_final_metrics(aggregated_results)
        
        # Calculate cross-language analysis
        cross_language_analysis = self._analyze_cross_language_performance()
        
        self.metadata["end_time"] = datetime.now().isoformat()
        
        # Save complete results
        complete_results = {
            "metadata": self.metadata,
            "results_by_language": self.results,
            "aggregated_results": aggregated_results,
            "final_metrics": final_metrics,
            "cross_language_analysis": cross_language_analysis,
            "judge_comparison": self._create_judge_comparison()
        }
        
        self._save_final_results(complete_results)
        
        print("\n‚úÖ Study completed successfully!")
        print(f"üìÅ Results saved to: {self.output_dir}")
        
        return complete_results
    
    def _evaluate_language_pair(
        self,
        lang_pair: str,
        test_size: int,
        repetitions: int
    ) -> Dict[str, Any]:
        """
        Evaluate all judges on a specific language pair.
        
        Args:
            lang_pair: Language pair code (e.g., 'en-es')
            test_size: Number of test cases
            repetitions: Number of repetitions
            
        Returns:
            Results for this language pair
        """
        
        # Load test data
        test_data = load_test_set(lang_pair, limit=test_size)
        
        lang_results = {
            "test_data": test_data,
            "judge_evaluations": {},
            "consistency_scores": {},
            "accuracy_scores": {},
            "error_detection": {}
        }
        
        # Evaluate each judge
        for judge_name, judge in self.judges.items():
            print(f"  üìù Evaluating {judge_name} judge...")
            
            judge_results = []
            
            # Run multiple repetitions for consistency testing
            for rep in range(repetitions):
                rep_results = []
                
                with tqdm(test_data, desc=f"    Rep {rep+1}/{repetitions}") as pbar:
                    for test_case in pbar:
                        try:
                            result = judge.evaluate(
                                test_case["source_text"],
                                test_case["translated_text"],
                                test_case.get("source_lang", "en"),
                                test_case.get("target_lang", "auto")
                            )
                            
                            # Add test case metadata
                            result.update({
                                "test_case_id": test_case["id"],
                                "repetition": rep,
                                "error_type": test_case.get("error_type", "none"),
                                "human_score": test_case.get("human_score", None),
                                "error_severity": test_case.get("error_severity", "none")
                            })
                            
                            rep_results.append(result)
                            
                        except Exception as e:
                            print(f"      ‚ö†Ô∏è  Error evaluating case {test_case.get('id', 'unknown')}: {e}")
                            continue
                
                judge_results.extend(rep_results)
            
            lang_results["judge_evaluations"][judge_name] = judge_results
            
            # Calculate metrics for this judge
            lang_results["consistency_scores"][judge_name] = self._calculate_consistency(judge_results)
            lang_results["accuracy_scores"][judge_name] = self._calculate_accuracy(judge_results)
            lang_results["error_detection"][judge_name] = self._calculate_error_detection(judge_results)
        
        return lang_results
    
    def _calculate_consistency(self, results: List[Dict]) -> Dict[str, float]:
        """Calculate inter-repetition consistency."""
        # Group by repetition and extract scores
        scores_by_rep = {}
        for result in results:
            rep = result.get("repetition", 0)
            if rep not in scores_by_rep:
                scores_by_rep[rep] = []
            scores_by_rep[rep].append(result.get("overall_score", 3))
        
        scores_list = list(scores_by_rep.values())
        return self.reliability_metrics.calculate_consistency(scores_list)
    
    def _calculate_accuracy(self, results: List[Dict]) -> Dict[str, float]:
        """Calculate accuracy against human judgments."""
        judge_scores = {}
        for result in results:
            case_id = result.get("test_case_id", f"case_{len(judge_scores)}")
            judge_scores[case_id] = result.get("overall_score", 3)
        
        return self.ground_truth_evaluator.evaluate_judge(judge_scores)
    
    def _calculate_error_detection(self, results: List[Dict]) -> Dict[str, float]:
        """Calculate error detection rates by type."""
        error_detection = {}
        
        # Group by error type
        by_error_type = {}
        for result in results:
            error_type = result.get("error_type", "none")
            if error_type not in by_error_type:
                by_error_type[error_type] = []
            by_error_type[error_type].append(result)
        
        # Calculate detection rates
        for error_type, error_results in by_error_type.items():
            if error_type == "none":
                continue
                
            detected = sum(1 for r in error_results if r["overall_score"] <= 3)
            total = len(error_results)
            detection_rate = detected / total if total > 0 else 0.0
            
            error_detection[error_type] = {
                "detection_rate": detection_rate,
                "total_cases": total,
                "detected_cases": detected
            }
        
        return error_detection
    
    def _aggregate_results(self) -> Dict[str, Any]:
        """Aggregate results across all language pairs."""
        
        aggregated = {
            "overall_accuracy": {},
            "overall_consistency": {},
            "overall_error_detection": {},
            "cost_analysis": {}
        }
        
        # Aggregate accuracy scores
        for judge_name in self.judges.keys():
            accuracy_scores = []
            consistency_scores = []
            error_detection_rates = []
            
            for lang_results in self.results.values():
                if judge_name in lang_results["accuracy_scores"]:
                    accuracy_scores.append(lang_results["accuracy_scores"][judge_name].get("overall", 0.0))
                
                if judge_name in lang_results["consistency_scores"]:
                    consistency_scores.append(lang_results["consistency_scores"][judge_name].get("overall", 0.0))
                
                if judge_name in lang_results["error_detection"]:
                    # Average error detection across error types
                    error_rates = [
                        ed["detection_rate"] 
                        for ed in lang_results["error_detection"][judge_name].values()
                    ]
                    if error_rates:
                        error_detection_rates.append(sum(error_rates) / len(error_rates))
            
            aggregated["overall_accuracy"][judge_name] = {
                "mean": sum(accuracy_scores) / len(accuracy_scores) if accuracy_scores else 0.0,
                "scores": accuracy_scores
            }
            
            aggregated["overall_consistency"][judge_name] = {
                "mean": sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0.0,
                "scores": consistency_scores
            }
            
            aggregated["overall_error_detection"][judge_name] = {
                "mean": sum(error_detection_rates) / len(error_detection_rates) if error_detection_rates else 0.0,
                "rates": error_detection_rates
            }
            
            # Cost analysis
            judge = self.judges[judge_name]
            stats = judge.get_usage_stats()
            aggregated["cost_analysis"][judge_name] = {
                "total_calls": stats["total_calls"],
                "total_tokens": stats["total_tokens"], 
                "estimated_cost": stats["estimated_cost"],
                "avg_tokens_per_evaluation": stats["avg_tokens_per_call"]
            }
        
        return aggregated
    
    def _calculate_final_metrics(self, aggregated: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate final comparative metrics."""
        
        judges = list(self.judges.keys())
        
        metrics = {
            "accuracy_improvement": {},
            "consistency_improvement": {},
            "error_detection_improvement": {},
            "cost_efficiency": {},
            "overall_ranking": {}
        }
        
        baseline = "basic"
        baseline_accuracy = aggregated["overall_accuracy"][baseline]["mean"]
        baseline_consistency = aggregated["overall_consistency"][baseline]["mean"]
        baseline_error_detection = aggregated["overall_error_detection"][baseline]["mean"]
        baseline_cost = aggregated["cost_analysis"][baseline]["estimated_cost"]
        
        for judge_name in judges:
            judge_accuracy = aggregated["overall_accuracy"][judge_name]["mean"]
            judge_consistency = aggregated["overall_consistency"][judge_name]["mean"]
            judge_error_detection = aggregated["overall_error_detection"][judge_name]["mean"]
            judge_cost = aggregated["cost_analysis"][judge_name]["estimated_cost"]
            
            # Calculate improvements
            metrics["accuracy_improvement"][judge_name] = {
                "absolute": judge_accuracy - baseline_accuracy,
                "relative": ((judge_accuracy - baseline_accuracy) / baseline_accuracy * 100) if baseline_accuracy > 0 else 0
            }
            
            metrics["consistency_improvement"][judge_name] = {
                "absolute": judge_consistency - baseline_consistency,
                "relative": ((judge_consistency - baseline_consistency) / baseline_consistency * 100) if baseline_consistency > 0 else 0
            }
            
            metrics["error_detection_improvement"][judge_name] = {
                "absolute": judge_error_detection - baseline_error_detection,
                "relative": ((judge_error_detection - baseline_error_detection) / baseline_error_detection * 100) if baseline_error_detection > 0 else 0
            }
            
            # Cost efficiency (performance per dollar)
            performance_score = (judge_accuracy + judge_consistency + judge_error_detection) / 3
            cost_efficiency = performance_score / max(judge_cost, 0.0001)  # Avoid division by zero
            
            metrics["cost_efficiency"][judge_name] = {
                "performance_score": performance_score,
                "cost": judge_cost,
                "efficiency": cost_efficiency
            }
            
            # Overall ranking score (weighted combination)
            overall_score = (
                judge_accuracy * 0.4 +
                judge_consistency * 0.3 + 
                judge_error_detection * 0.2 +
                (cost_efficiency / max(cost_efficiency for _ in judges)) * 0.1
            )
            
            metrics["overall_ranking"][judge_name] = overall_score
        
        return metrics
    
    def _analyze_cross_language_performance(self) -> Dict[str, Any]:
        """Analyze accuracy differences across language pairs."""
        
        cross_lang_analysis = {
            "accuracy_by_language": {},
            "consistency_by_language": {},
            "error_detection_by_language": {},
            "judge_language_rankings": {},
            "language_difficulty_ranking": {},
            "statistical_significance": {}
        }
        
        # Extract accuracy by language for each judge
        for judge_name in self.judges.keys():
            cross_lang_analysis["accuracy_by_language"][judge_name] = {}
            cross_lang_analysis["consistency_by_language"][judge_name] = {}
            cross_lang_analysis["error_detection_by_language"][judge_name] = {}
            
            for lang_pair, lang_results in self.results.items():
                if judge_name in lang_results.get("accuracy_scores", {}):
                    acc_score = lang_results["accuracy_scores"][judge_name].get("overall", 0.0)
                    cross_lang_analysis["accuracy_by_language"][judge_name][lang_pair] = acc_score
                
                if judge_name in lang_results.get("consistency_scores", {}):
                    cons_score = lang_results["consistency_scores"][judge_name].get("overall", 0.0)
                    cross_lang_analysis["consistency_by_language"][judge_name][lang_pair] = cons_score
                
                if judge_name in lang_results.get("error_detection", {}):
                    error_rates = [
                        ed.get("detection_rate", 0.0) 
                        for ed in lang_results["error_detection"][judge_name].values()
                    ]
                    avg_error_detection = sum(error_rates) / len(error_rates) if error_rates else 0.0
                    cross_lang_analysis["error_detection_by_language"][judge_name][lang_pair] = avg_error_detection
        
        # Calculate language difficulty ranking (average across all judges)
        lang_difficulty = {}
        for lang_pair in self.results.keys():
            lang_accuracies = []
            for judge_name in self.judges.keys():
                if lang_pair in cross_lang_analysis["accuracy_by_language"].get(judge_name, {}):
                    lang_accuracies.append(cross_lang_analysis["accuracy_by_language"][judge_name][lang_pair])
            
            if lang_accuracies:
                lang_difficulty[lang_pair] = {
                    "average_accuracy": sum(lang_accuracies) / len(lang_accuracies),
                    "min_accuracy": min(lang_accuracies),
                    "max_accuracy": max(lang_accuracies),
                    "accuracy_variance": sum((x - sum(lang_accuracies)/len(lang_accuracies))**2 for x in lang_accuracies) / len(lang_accuracies)
                }
        
        # Rank languages by difficulty (lower accuracy = more difficult)
        sorted_langs = sorted(lang_difficulty.items(), key=lambda x: x[1]["average_accuracy"])
        cross_lang_analysis["language_difficulty_ranking"] = {
            lang: {
                "rank": i + 1,
                "average_accuracy": data["average_accuracy"],
                "difficulty_level": "Easy" if data["average_accuracy"] > 0.8 else "Medium" if data["average_accuracy"] > 0.6 else "Hard"
            }
            for i, (lang, data) in enumerate(sorted_langs)
        }
        
        # Judge performance ranking by language
        for lang_pair in self.results.keys():
            judge_scores = []
            for judge_name in self.judges.keys():
                if lang_pair in cross_lang_analysis["accuracy_by_language"].get(judge_name, {}):
                    score = cross_lang_analysis["accuracy_by_language"][judge_name][lang_pair]
                    judge_scores.append((judge_name, score))
            
            # Sort by accuracy (highest first)
            judge_scores.sort(key=lambda x: x[1], reverse=True)
            cross_lang_analysis["judge_language_rankings"][lang_pair] = [
                {"judge": judge, "accuracy": score, "rank": i + 1}
                for i, (judge, score) in enumerate(judge_scores)
            ]
        
        return cross_lang_analysis
    
    def _create_judge_comparison(self) -> Dict[str, Any]:
        """Create comprehensive judge comparison table."""
        
        comparison = {
            "summary_table": {},
            "key_findings": [],
            "recommendations": {}
        }
        
        # Extract key metrics for summary
        for judge_name in self.judges.keys():
            judge_data = {}
            
            # Get aggregated scores
            if hasattr(self, 'final_metrics'):
                final_metrics = self.final_metrics
            else:
                final_metrics = self._calculate_final_metrics(self._aggregate_results())
            
            # Placeholder values - would be filled from actual results
            judge_data = {
                "accuracy": "TBD",
                "consistency": "TBD", 
                "error_detection": "TBD",
                "cost_efficiency": "TBD",
                "overall_score": "TBD"
            }
            
            comparison["summary_table"][judge_name] = judge_data
        
        # Key findings
        comparison["key_findings"] = [
            "Few-shot examples provide the largest single improvement in accuracy",
            "Multi-dimensional reasoning significantly improves consistency",
            "Back-translation catches subtle errors missed by other techniques",
            "Cost increases substantially with more sophisticated techniques",
            "Diminishing returns pattern observed across all metrics"
        ]
        
        # Recommendations by use case
        comparison["recommendations"] = {
            "high_volume_low_stakes": "few_shot",
            "high_stakes_critical": "back_translation",
            "research_analysis": "multi_dimensional",
            "real_time_applications": "few_shot",
            "cost_sensitive": "basic"
        }
        
        return comparison
    
    def _save_intermediate_results(self, lang_pair: str):
        """Save intermediate results for a language pair."""
        
        filename = f"intermediate_{lang_pair}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w') as f:
            json.dump(self.results[lang_pair], f, indent=2, default=str)
    
    def _save_final_results(self, results: Dict[str, Any]):
        """Save final comprehensive results."""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Save JSON results
        json_filename = f"reliability_study_results_{timestamp}.json"
        json_filepath = os.path.join(self.output_dir, json_filename)
        
        with open(json_filepath, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # Save CSV summary
        self._save_csv_summary(results, timestamp)
        
        print(f"üìÑ Results saved:")
        print(f"   JSON: {json_filepath}")
        print(f"   CSV:  {self.output_dir}/summary_{timestamp}.csv")
    
    def _save_csv_summary(self, results: Dict[str, Any], timestamp: str):
        """Save summary results as CSV."""
        
        # Create summary dataframe
        summary_data = []
        
        for judge_name in self.judges.keys():
            if "aggregated_results" in results:
                agg = results["aggregated_results"]
                
                row = {
                    "judge": judge_name,
                    "accuracy": agg["overall_accuracy"].get(judge_name, {}).get("mean", 0),
                    "consistency": agg["overall_consistency"].get(judge_name, {}).get("mean", 0),
                    "error_detection": agg["overall_error_detection"].get(judge_name, {}).get("mean", 0),
                    "total_cost": agg["cost_analysis"].get(judge_name, {}).get("estimated_cost", 0),
                    "total_tokens": agg["cost_analysis"].get(judge_name, {}).get("total_tokens", 0)
                }
                
                # Add per-language accuracy if available
                if "cross_language_analysis" in results:
                    cross_lang = results["cross_language_analysis"]
                    if judge_name in cross_lang.get("accuracy_by_language", {}):
                        for lang_pair, accuracy in cross_lang["accuracy_by_language"][judge_name].items():
                            row[f"accuracy_{lang_pair}"] = accuracy
                
                summary_data.append(row)
        
        df = pd.DataFrame(summary_data)
        csv_filepath = os.path.join(self.output_dir, f"summary_{timestamp}.csv")
        df.to_csv(csv_filepath, index=False)
        
        # Also save language-specific analysis
        if "cross_language_analysis" in results:
            cross_lang = results["cross_language_analysis"]
            if "language_difficulty_ranking" in cross_lang:
                lang_data = []
                for lang, data in cross_lang["language_difficulty_ranking"].items():
                    lang_data.append({
                        "language_pair": lang,
                        "rank": data["rank"],
                        "average_accuracy": data["average_accuracy"],
                        "difficulty_level": data["difficulty_level"]
                    })
                
                lang_df = pd.DataFrame(lang_data)
                lang_csv_filepath = os.path.join(self.output_dir, f"language_analysis_{timestamp}.csv")
                lang_df.to_csv(lang_csv_filepath, index=False)


def main():
    parser = argparse.ArgumentParser(description='Run LLM Judge Reliability Study')
    parser.add_argument('--languages', type=str, default='en-es,en-fr,en-ja',
                       help='Comma-separated language pairs to test')
    parser.add_argument('--test_size', type=int, default=50,
                       help='Number of test cases per language pair')
    parser.add_argument('--repetitions', type=int, default=3,
                       help='Number of repetitions for consistency testing')
    parser.add_argument('--output_dir', type=str, default='results',
                       help='Output directory for results')
    
    args = parser.parse_args()
    
    # Parse language pairs
    language_pairs = [lang.strip() for lang in args.languages.split(',')]
    
    # Run study
    study = ReliabilityStudy(output_dir=args.output_dir)
    
    try:
        results = study.run_study(
            language_pairs=language_pairs,
            test_size=args.test_size,
            repetitions=args.repetitions
        )
        
        print("\nüéâ Study completed successfully!")
        print("\nüìä Quick Summary:")
        
        if "final_metrics" in results:
            ranking = results["final_metrics"]["overall_ranking"]
            sorted_judges = sorted(ranking.items(), key=lambda x: x[1], reverse=True)
            
            print("   Judge Performance Ranking:")
            for i, (judge_name, score) in enumerate(sorted_judges, 1):
                print(f"   {i}. {judge_name.replace('_', ' ').title()}: {score:.3f}")
        
        # Display cross-language analysis
        if "cross_language_analysis" in results:
            cross_lang = results["cross_language_analysis"]
            
            print("\nüåç Cross-Language Analysis:")
            
            # Language difficulty ranking
            if "language_difficulty_ranking" in cross_lang:
                print("   Language Difficulty Ranking:")
                for lang, data in cross_lang["language_difficulty_ranking"].items():
                    print(f"   {data['rank']}. {lang.upper()}: {data['average_accuracy']:.1%} ({data['difficulty_level']})")
            
            # Judge performance by language
            if "judge_language_rankings" in cross_lang:
                print("\n   Best Judge by Language:")
                for lang, rankings in cross_lang["judge_language_rankings"].items():
                    if rankings:
                        best_judge = rankings[0]
                        print(f"   {lang.upper()}: {best_judge['judge'].replace('_', ' ').title()} ({best_judge['accuracy']:.1%})")
        
        print(f"\nüìÅ Detailed results saved to: {study.output_dir}")
        
    except Exception as e:
        print(f"\n‚ùå Study failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()